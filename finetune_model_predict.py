# finetune_model_predict.py
"""
Nucleotide Transformeræ¨¡å‹é¢„æµ‹è„šæœ¬ - æœ¬åœ°åŠ è½½ç‰ˆæœ¬
å®Œå…¨é¿å…ç½‘ç»œè¿æ¥ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹æ–‡ä»¶
"""
import argparse
import os
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoModel, AutoTokenizer, AutoConfig
from tqdm.auto import tqdm
import numpy as np
import json

# ---------------------
# æ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
# ---------------------
class NTClassificationModel(nn.Module):
    def __init__(self, backbone, num_labels=2, dropout=0.3):
        super().__init__()
        self.backbone = backbone
        hidden_size = backbone.config.hidden_size
        
        self.pooling = nn.AdaptiveAvgPool1d(1)
        
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_size, 512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(512, 128),
            nn.GELU(), 
            nn.Dropout(dropout),
            nn.Linear(128, num_labels)
        )

    def forward(self, input_ids=None, attention_mask=None, **kwargs):
        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        
        if hasattr(outputs, 'last_hidden_state'):
            last_hidden = outputs.last_hidden_state
        else:
            last_hidden = outputs[0]
            
        avg_pool = last_hidden.mean(dim=1)
        max_pool = last_hidden.max(dim=1)[0]
        pooled_output = avg_pool + max_pool
        
        logits = self.classifier(pooled_output)
        return logits

# ---------------------
# æœ¬åœ°æ¨¡å‹åŠ è½½å‡½æ•°
# ---------------------
def load_model_and_tokenizer_locally(model_path, device):
    """
    ä»æœ¬åœ°ç›®å½•åŠ è½½æ¨¡å‹å’Œtokenizerï¼Œå®Œå…¨é¿å…ç½‘ç»œè¿æ¥
    """
    print("ğŸ”§ æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹å’Œtokenizer...")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯HuggingFaceæ¨¡å‹ç›®å½•
    if os.path.isdir(model_path):
        model_dir = model_path
    else:
        # å¦‚æœæ˜¯æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå°è¯•æ‰¾åˆ°å¯¹åº”çš„æ¨¡å‹ç›®å½•
        checkpoint_dir = os.path.dirname(model_path)
        # åœ¨æ£€æŸ¥ç‚¹ç›®å½•ä¸­æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
        possible_dirs = [
            os.path.join(checkpoint_dir, "model"),
            checkpoint_dir,
            os.path.dirname(checkpoint_dir)
        ]
        
        model_dir = None
        for dir_path in possible_dirs:
            if os.path.exists(os.path.join(dir_path, "config.json")):
                model_dir = dir_path
                break
        
        if model_dir is None:
            print("âŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
            print("è¯·è¿è¡Œ: python download_model.py æ¥ä¸‹è½½æ¨¡å‹")
            return None, None
    
    # åŠ è½½tokenizer
    try:
        # é¦–å…ˆå°è¯•ä»æœ¬åœ°åŠ è½½
        tokenizer = AutoTokenizer.from_pretrained(
            model_dir,
            local_files_only=True,
            trust_remote_code=True
        )
        print("âœ… Tokenizeræœ¬åœ°åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Tokenizeræœ¬åœ°åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å·²å®Œæ•´ä¸‹è½½åˆ°æœ¬åœ°")
        return None, None
    
    # åŠ è½½æ¨¡å‹é…ç½®
    try:
        config = AutoConfig.from_pretrained(model_dir, local_files_only=True)
        backbone = AutoModel.from_pretrained(
            model_dir,
            config=config,
            local_files_only=True,
            trust_remote_code=True
        )
        print("âœ… æ¨¡å‹æœ¬åœ°åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹æœ¬åœ°åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    return backbone, tokenizer

def download_model_locally(model_name="InstaDeepAI/nucleotide-transformer-500m-1000g", local_dir="./local_models"):
    """
    ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç›®å½•çš„è¾…åŠ©å‡½æ•°
    """
    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹ {model_name} åˆ°æœ¬åœ°ç›®å½• {local_dir}...")
    
    os.makedirs(local_dir, exist_ok=True)
    model_dir = os.path.join(local_dir, model_name.split('/')[-1])
    
    try:
        # ä¸‹è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            cache_dir=local_dir
        )
        tokenizer.save_pretrained(model_dir)
        print("âœ… Tokenizerä¸‹è½½å¹¶ä¿å­˜æˆåŠŸ")
        
        # ä¸‹è½½æ¨¡å‹
        model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            cache_dir=local_dir
        )
        model.save_pretrained(model_dir)
        print("âœ… æ¨¡å‹ä¸‹è½½å¹¶ä¿å­˜æˆåŠŸ")
        
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir}")
        return model_dir
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return None

# ---------------------
# æ•°æ®å¤„ç†
# ---------------------
class SeqDataset(torch.utils.data.Dataset):
    def __init__(self, sequences, tokenizer, max_length=512):
        self.seqs = sequences
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.seqs)

    def __getitem__(self, idx):
        seq = self.seqs[idx]
        
        enc = self.tokenizer(
            seq,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        item = {k: v.squeeze(0) for k, v in enc.items()}
        return item

def load_sequences_from_csv(csv_path):
    """ä»CSVæ–‡ä»¶åŠ è½½åºåˆ—"""
    df = pd.read_csv(csv_path)
    
    # è‡ªåŠ¨æ£€æµ‹åºåˆ—åˆ—
    sequence_columns = [col for col in df.columns if 'sequence' in col.lower() or 'seq' in col.lower() or 'dna' in col.lower()]
    if sequence_columns:
        seq_col = sequence_columns[0]
        print(f"æ£€æµ‹åˆ°åºåˆ—åˆ—: {seq_col}")
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åºåˆ—ç›¸å…³çš„åˆ—åï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—
        seq_col = df.columns[0]
        print(f"ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºåºåˆ—: {seq_col}")
    
    sequences = df[seq_col].astype(str).tolist()
    return sequences, df

# ---------------------
# é¢„æµ‹å‡½æ•°
# ---------------------
def predict(model, dataloader, device):
    """è¿›è¡Œé¢„æµ‹"""
    model.eval()
    all_probs = []
    all_logits = []
    all_predictions = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="é¢„æµ‹ä¸­"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            
            logits = model(input_ids=input_ids, attention_mask=attention_mask)
            probs = torch.softmax(logits, dim=-1)
            
            all_probs.extend(probs.cpu().numpy())
            all_logits.extend(logits.cpu().numpy())
            all_predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())
    
    return all_probs, all_logits, all_predictions

# ---------------------
# æ¨¡å‹åŠ è½½
# ---------------------
def load_trained_model(checkpoint_path, local_model_dir, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆå®Œå…¨æœ¬åœ°ï¼‰"""
    print(f"åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # åŠ è½½backboneï¼ˆæœ¬åœ°ï¼‰
    backbone, tokenizer = load_model_and_tokenizer_locally(local_model_dir, device)
    if backbone is None:
        return None, None
    
    # åˆ›å»ºæ¨¡å‹æ¶æ„
    model = NTClassificationModel(backbone, num_labels=2)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint["model_state_dict"])
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    model.to(device)
    model.eval()
    
    return model, tokenizer

# ---------------------
# ä¸»å‡½æ•°
# ---------------------
def main():
    parser = argparse.ArgumentParser(description="Nucleotide Transformeræ¨¡å‹é¢„æµ‹ - æœ¬åœ°ç‰ˆæœ¬")
    parser.add_argument("--checkpoint", type=str, required=True, 
                       help="è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--input_csv", type=str, required=True,
                       help="è¾“å…¥æ•°æ®CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_csv", type=str, required=True,
                       help="é¢„æµ‹ç»“æœè¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--local_model_dir", type=str, 
                       default="./local_models/nucleotide-transformer-500m-1000g",
                       help="æœ¬åœ°æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=16,
                       help="é¢„æµ‹æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_length", type=int, default=512,
                       help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--device", type=str, default="cuda",
                       help="æ¨ç†è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--download_model", action="store_true",
                       help="å¦‚æœæœ¬åœ°æ²¡æœ‰æ¨¡å‹ï¼Œå…ˆä¸‹è½½æ¨¡å‹")
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.checkpoint):
        print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    if not os.path.exists(args.input_csv):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_csv}")
        return
    
    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨ä¸”æŒ‡å®šäº†ä¸‹è½½ï¼Œåˆ™å…ˆä¸‹è½½
    if not os.path.exists(args.local_model_dir) and args.download_model:
        print("æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
        model_dir = download_model_locally(local_dir=os.path.dirname(args.local_model_dir))
        if model_dir is None:
            return
    elif not os.path.exists(args.local_model_dir):
        print(f"âŒ æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {args.local_model_dir}")
        print("è¯·ä½¿ç”¨ --download_model å‚æ•°è‡ªåŠ¨ä¸‹è½½ï¼Œæˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°è¯¥ç›®å½•")
        return
    
    # åŠ è½½æ¨¡å‹å’Œtokenizerï¼ˆå®Œå…¨æœ¬åœ°ï¼‰
    model, tokenizer = load_trained_model(args.checkpoint, args.local_model_dir, device)
    if model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return
    
    # åŠ è½½æ•°æ®
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    sequences, original_df = load_sequences_from_csv(args.input_csv)
    print(f"åŠ è½½äº† {len(sequences)} æ¡åºåˆ—")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = SeqDataset(sequences, tokenizer, args.max_length)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)
    
    # è¿›è¡Œé¢„æµ‹
    print("å¼€å§‹é¢„æµ‹...")
    probabilities, logits, predictions = predict(model, dataloader, device)
    
    # å‡†å¤‡è¾“å‡ºç»“æœ
    results_df = original_df.copy()
    
    # æ·»åŠ é¢„æµ‹ç»“æœ
    results_df['prediction'] = predictions
    results_df['probability_class_0'] = [prob[0] for prob in probabilities]
    results_df['probability_class_1'] = [prob[1] for prob in probabilities]
    results_df['confidence'] = np.max(probabilities, axis=1)
    
    # æ·»åŠ é¢„æµ‹æ ‡ç­¾ï¼ˆå¯æ ¹æ®éœ€è¦è‡ªå®šä¹‰ï¼‰
    results_df['predicted_label'] = results_df['prediction'].map({0: 'negative', 1: 'positive'})
    
    # ä¿å­˜ç»“æœ
    results_df.to_csv(args.output_csv, index=False)
    print(f"âœ… é¢„æµ‹å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {args.output_csv}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(results_df)}")
    print(f"   é¢„æµ‹ä¸ºæ­£ç±»çš„æ ·æœ¬æ•°: {sum(predictions)}")
    print(f"   é¢„æµ‹ä¸ºè´Ÿç±»çš„æ ·æœ¬æ•°: {len(predictions) - sum(predictions)}")
    print(f"   å¹³å‡ç½®ä¿¡åº¦: {np.mean(results_df['confidence']):.4f}")

# ---------------------
# ä¸‹è½½æ¨¡å‹çš„ç‹¬ç«‹è„šæœ¬
# ---------------------
def download_model_script():
    """ç‹¬ç«‹çš„æ¨¡å‹ä¸‹è½½è„šæœ¬"""
    model_name = "InstaDeepAI/nucleotide-transformer-500m-1000g"
    local_dir = "./local_models"
    
    print("ğŸš€ å¼€å§‹ä¸‹è½½Nucleotide Transformeræ¨¡å‹...")
    model_path = download_model_locally(model_name, local_dir)
    
    if model_path:
        print(f"\nğŸ‰ æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {model_path}")
        print(f"\nä½¿ç”¨ç¤ºä¾‹:")
        print(f"python finetune_model_predict.py --checkpoint your_checkpoint.pt --input_csv test.csv --output_csv predictions.csv --local_model_dir {model_path}")
    else:
        print("âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")

if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    import sys
    if len(sys.argv) == 1:
        print("Nucleotide Transformeræœ¬åœ°é¢„æµ‹è„šæœ¬")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("1. é¦–å…ˆä¸‹è½½æ¨¡å‹:")
        print("   python finetune_model_predict.py --download_model")
        print("\n2. ç„¶åè¿›è¡Œé¢„æµ‹:")
        print("   python finetune_model_predict.py --checkpoint your_model.pt --input_csv test.csv --output_csv predictions.csv")
        print("\n3. æˆ–è€…ä¸€æ­¥å®Œæˆï¼ˆè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼‰:")
        print("   python finetune_model_predict.py --checkpoint your_model.pt --input_csv test.csv --output_csv predictions.csv --download_model")
        sys.exit(1)
    
    main()